{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4a56d3-f2dc-4486-b8fc-518d7553fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b71cf5ec204230b80187d60c143258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load pretrained LDM\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "vae = pipe.vae.to(\"cuda\")\n",
    "unet = pipe.unet.to(\"cuda\")  # Weâ€™ll fine-tune this\n",
    "scheduler = pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dce5c-7ecf-4554-a9b8-82236daaa013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1467 Indian images (race=3)\n",
      "Epoch 0, Step 0, Loss: 0.0047\n",
      "Epoch 0, Step 10, Loss: 0.0111\n",
      "Epoch 0, Step 20, Loss: 0.0158\n",
      "Epoch 0, Step 30, Loss: 0.0052\n",
      "Epoch 0, Step 40, Loss: 0.0948\n",
      "Epoch 0, Step 50, Loss: 0.0800\n",
      "Epoch 0, Step 60, Loss: 0.0894\n",
      "Epoch 0, Step 70, Loss: 0.3219\n",
      "Epoch 0, Step 80, Loss: 0.1315\n",
      "Epoch 0, Step 90, Loss: 0.5133\n",
      "Epoch 0, Step 100, Loss: 0.0843\n",
      "Epoch 0, Step 110, Loss: 0.1255\n",
      "Epoch 0, Step 120, Loss: 0.0316\n",
      "Epoch 0, Step 130, Loss: 0.0084\n",
      "Epoch 0, Step 140, Loss: 0.1827\n",
      "Epoch 0, Step 150, Loss: 0.0555\n",
      "Epoch 0, Step 160, Loss: 0.0889\n",
      "Epoch 0, Step 170, Loss: 0.0068\n",
      "Epoch 0, Step 180, Loss: 0.0102\n",
      "Epoch 0, Step 190, Loss: 0.0003\n",
      "Epoch 0, Step 200, Loss: 0.1004\n",
      "Epoch 0, Step 210, Loss: 0.0168\n",
      "Epoch 0, Step 220, Loss: 0.0163\n",
      "Epoch 0, Step 230, Loss: 0.1067\n",
      "Epoch 0, Step 240, Loss: 0.4050\n",
      "Epoch 0, Step 250, Loss: 0.0048\n",
      "Epoch 0, Step 260, Loss: 0.0005\n",
      "Epoch 0, Step 270, Loss: 0.3861\n",
      "Epoch 0, Step 280, Loss: 0.0116\n",
      "Epoch 0, Step 290, Loss: 0.4804\n",
      "Epoch 0, Step 300, Loss: 0.0170\n",
      "Epoch 0, Step 310, Loss: 0.0011\n",
      "Epoch 0, Step 320, Loss: 0.0350\n",
      "Epoch 0, Step 330, Loss: 0.0403\n",
      "Epoch 0, Step 340, Loss: 0.0072\n",
      "Epoch 0, Step 350, Loss: 0.3059\n",
      "Epoch 0, Step 360, Loss: 0.2804\n",
      "Epoch 0, Step 370, Loss: 0.0174\n",
      "Epoch 0, Step 380, Loss: 0.0003\n",
      "Epoch 0, Step 390, Loss: 0.0011\n",
      "Epoch 0, Step 400, Loss: 0.2458\n",
      "Epoch 0, Step 410, Loss: 0.0421\n",
      "Epoch 0, Step 420, Loss: 0.1881\n",
      "Epoch 0, Step 430, Loss: 0.0080\n",
      "Epoch 0, Step 440, Loss: 0.1242\n",
      "Epoch 0, Step 450, Loss: 0.0162\n",
      "Epoch 0, Step 460, Loss: 0.4097\n",
      "Epoch 0, Step 470, Loss: 0.1642\n",
      "Epoch 0, Step 480, Loss: 0.2409\n",
      "Epoch 0, Step 490, Loss: 0.0384\n",
      "Epoch 0, Step 500, Loss: 0.0360\n",
      "Epoch 0, Step 510, Loss: 0.2370\n",
      "Epoch 0, Step 520, Loss: 0.0001\n",
      "Epoch 0, Step 530, Loss: 0.1745\n",
      "Epoch 0, Step 540, Loss: 0.0377\n",
      "Epoch 0, Step 550, Loss: 0.2596\n",
      "Epoch 0, Step 560, Loss: 0.1066\n",
      "Epoch 0, Step 570, Loss: 0.0245\n",
      "Epoch 0, Step 580, Loss: 0.0060\n",
      "Epoch 0, Step 590, Loss: 0.3821\n",
      "Epoch 0, Step 600, Loss: 0.0617\n",
      "Epoch 0, Step 610, Loss: 0.0041\n",
      "Epoch 0, Step 620, Loss: 0.2019\n",
      "Epoch 0, Step 630, Loss: 0.0037\n",
      "Epoch 0, Step 640, Loss: 0.0007\n",
      "Epoch 0, Step 650, Loss: 0.0211\n",
      "Epoch 0, Step 660, Loss: 0.1037\n",
      "Epoch 0, Step 670, Loss: 0.0821\n",
      "Epoch 0, Step 680, Loss: 0.0029\n",
      "Epoch 0, Step 690, Loss: 0.0121\n",
      "Epoch 0, Step 700, Loss: 0.5341\n",
      "Epoch 0, Step 710, Loss: 0.0098\n",
      "Epoch 0, Step 720, Loss: 0.0006\n",
      "Epoch 0, Step 730, Loss: 0.0083\n",
      "Epoch 1, Step 0, Loss: 0.0238\n",
      "Epoch 1, Step 10, Loss: 0.0481\n",
      "Epoch 1, Step 20, Loss: 0.0220\n",
      "Epoch 1, Step 30, Loss: 0.0126\n",
      "Epoch 1, Step 40, Loss: 0.0063\n",
      "Epoch 1, Step 50, Loss: 0.3245\n",
      "Epoch 1, Step 60, Loss: 0.3171\n",
      "Epoch 1, Step 70, Loss: 0.0010\n",
      "Epoch 1, Step 80, Loss: 0.0020\n",
      "Epoch 1, Step 90, Loss: 0.0966\n",
      "Epoch 1, Step 100, Loss: 0.0461\n",
      "Epoch 1, Step 110, Loss: 0.0278\n",
      "Epoch 1, Step 120, Loss: 0.0108\n",
      "Epoch 1, Step 130, Loss: 0.0156\n",
      "Epoch 1, Step 140, Loss: 0.0050\n",
      "Epoch 1, Step 150, Loss: 0.5009\n",
      "Epoch 1, Step 160, Loss: 0.0046\n",
      "Epoch 1, Step 170, Loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, DDIMScheduler, AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-5\n",
    "num_epochs = 15\n",
    "denoise_strength = 0.7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# IndianFacesDataset (unchanged)\n",
    "class IndianFacesDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, race_filter=3):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Folder {img_dir} does not exist!\")\n",
    "        \n",
    "        self.img_names = [f for f in os.listdir(img_dir) if f.endswith('jpg.chip.jpg')]\n",
    "        self.metadata = []\n",
    "        for img_name in self.img_names:\n",
    "            parts = img_name.split(\"_\")\n",
    "            try:\n",
    "                age, gender, race = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "                if race == race_filter:\n",
    "                    self.metadata.append({\n",
    "                        \"filename\": img_name, \n",
    "                        \"age\": age,\n",
    "                        \"gender\": gender\n",
    "                    })\n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(f\"Skipping {img_name}: Invalid format ({e})\")\n",
    "        \n",
    "        print(f\"Found {len(self.metadata)} Indian images (race=3)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.metadata[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        age = torch.tensor(img_info[\"age\"], dtype=torch.float32) / 116.0\n",
    "        gender = torch.tensor(img_info[\"gender\"], dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"age\": age,\n",
    "            \"gender\": gender,\n",
    "            \"filename\": img_info[\"filename\"]\n",
    "        }\n",
    "\n",
    "# Modified FaceAgingDataset\n",
    "class FaceAgingDataset(IndianFacesDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        data = super().__getitem__(idx)\n",
    "        \n",
    "        gender = data[\"gender\"].item()\n",
    "        age_years = int(data[\"age\"].item() * 116)\n",
    "        gender_str = \"male\" if gender == 0 else \"female\"\n",
    "        prompt = f\"A high-quality photo of an Indian {gender_str}, age {age_years} years\"\n",
    "        \n",
    "        return {\n",
    "            \"image\": data[\"image\"],\n",
    "            \"age\": data[\"age\"],\n",
    "            \"gender\": data[\"gender\"],\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "\n",
    "# AgeAdapter (unchanged)\n",
    "class AgeAdapter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, 768),\n",
    "            nn.LayerNorm(768)\n",
    "        )\n",
    "        \n",
    "    def forward(self, age):\n",
    "        return self.proj(age.unsqueeze(-1))\n",
    "\n",
    "# Training Loop\n",
    "def train_face_aging(num_epochs=15, lr=1e-5, batch_size=2, image_size=512):\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\").to(device)\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\").to(device)\n",
    "    age_adapter = AgeAdapter().to(device)\n",
    "\n",
    "    text_encoder.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        list(unet.parameters()) + list(age_adapter.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    dataset = FaceAgingDataset(\"indian_images\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    noise_scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        beta_schedule=\"linear\"\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            ages = batch[\"age\"].to(device)\n",
    "            genders = batch[\"gender\"].to(device)\n",
    "            prompts = batch[\"prompt\"]\n",
    "            \n",
    "            text_input = tokenizer(\n",
    "                prompts,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            text_input = {k: v.to(device) for k, v in text_input.items()}\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = text_encoder(text_input[\"input_ids\"])[0]\n",
    "            \n",
    "            gender_embeddings = age_adapter(genders.float().unsqueeze(1)).squeeze(1)\n",
    "            age_emb = age_adapter(ages.unsqueeze(1))\n",
    "            combined_embeddings = text_embeddings + age_emb + gender_embeddings.unsqueeze(1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "            \n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps,\n",
    "                (latents.shape[0],), device=device\n",
    "            ).long()\n",
    "            \n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=combined_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    unet.save_pretrained(\"./face_aging_unet\")\n",
    "    torch.save(age_adapter.state_dict(), \"./age_adapter.pth\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Face Aging Inference Pipeline\n",
    "class FaceAgingPipeline(StableDiffusionImg2ImgPipeline):\n",
    "    def __init__(self, *args, age_adapter_path=\"./age_adapter.pth\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.age_adapter = AgeAdapter().to(self.device)\n",
    "        self.age_adapter.load_state_dict(torch.load(age_adapter_path))\n",
    "        \n",
    "    def __call__(self, image, target_age, strength=0.7, guidance_scale=7.5):\n",
    "        prompt = f\"A high-quality photo of an Indian person, age {target_age} years\"\n",
    "        \n",
    "        text_input = self.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        text_input = {k: v.to(self.device) for k, v in text_input.items()}\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(text_input[\"input_ids\"])[0]\n",
    "        \n",
    "        age_tensor = torch.tensor([[target_age / 116.0]], device=self.device)\n",
    "        age_emb = self.age_adapter(age_tensor)\n",
    "        combined_embeddings = text_embeddings + age_emb\n",
    "        \n",
    "        return super().__call__(\n",
    "            prompt_embeds=combined_embeddings,\n",
    "            image=image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale\n",
    "        )\n",
    "\n",
    "# Usage Example\n",
    "def perform_face_aging(input_image_path, target_age):\n",
    "    pipe = FaceAgingPipeline.from_pretrained(\n",
    "        \"face_aging_unet\",\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "        age_adapter_path=\"./age_adapter.pth\"\n",
    "    ).to(device)\n",
    "    \n",
    "    input_image = Image.open(input_image_path).convert(\"RGB\").resize((512, 512))\n",
    "    \n",
    "    result = pipe(\n",
    "        image=input_image,\n",
    "        target_age=target_age,\n",
    "        strength=0.75,\n",
    "        guidance_scale=8.0\n",
    "    )\n",
    "    \n",
    "    return result.images[0]\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Training\n",
    "    train_face_aging()\n",
    "    \n",
    "    # Inference\n",
    "    aged_image = perform_face_aging(\"input.jpg\", target_age=50)\n",
    "    aged_image.save(\"output.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39391023-6678-4df9-9759-5d0da98d5564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
